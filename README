Synopsis:
	A trivial protocol to manage immutable blobs over a network.
Description:
	The blobio system implements a simple client/server protocol for 
	associating crypto hash digests as keys for immutable, binary large
	objects stored on a network.  The blobs are referenced with a uri
	(uniform resource identifier) called a uniform digest, shortened to
	'udig', with a syntax like

		algorithm:digest

	where 'algorithm' is the hash algorithm, like 'sha' or 'btc20', for
	example, and 'digest' is the ascii hash value of the blob.  The colon
	character separates the algorithm and the digest.  The algorithm
	matches the perl5 regex
	
		^[a-z][a-z0-9]{0,7}$

	and the digest matches both perl5 regexs

		^[[:graph:]]{32,128}$
		^[[:ascii:]{32,128}$

	Currently, just two digests are supported:

		sha:c59f2c4f35b56a4bdf2d2c9f4a9984f2049cf2d4
		btc20:b0e889a067b5d4fff15c3d6e646554939a793783

	where "sha" is the famous but broken SHA1 and "btc20" is a
	composition of RIPEMD(SHA256(SHA256(blob))), inspired by the
	bitcoin wallet.  Eventually a SHA3 digest can be added without
	breaking code.  The experimental digest "bc160" will be gone soon.

	An example udig is sha:cd50d19784897085a8d0e3e413f8612b097c03f1
	which describes the string "hello, world\n".

		echo 'hello, world' | shasum
		cd50d19784897085a8d0e3e413f8612b097c03f1  -

	The maximum size of the algorithm tag is 8 characters.  The maximum size
	of a digest is 128 characters, making the maximum size of a udig =
	8 + 128 + 1 (colon).  The minimum size of a digest is 32 ascii chars,
	making the minum size of a udig 1 + 1 + 32 = 34 chars.

	The network protocol implements 7 verbs
	
		get <udig>		#  read a blob with a particular digest
		put <udig>		#  write a blob with a digest
		take <udig>		#  get a blob that the source may forget
		give <udig>		#  put a blob that the source may forget
		eat <udig>		#  untrusted digest of a blob by server

		wrap			#  freeze current unwrapped blob traffic
					#  log into a single blob and return
					#  udig of the set of all blobs wrapped
					#  since the previous "roll". the
					#  wrapped <udig>, returned to client,
					#  will  be in the next wrap, which
					#  allows perpetual merkle chaining.

		roll <udig>		#  forget all wrapped blob traffic logs
					#  in the udig set described by <udig>.
					#  subsequent wrap/rolls will never see 
					#  these blob sets again.
					#
					#  however, the blobs themselves will
					#  exist and probably be gettable.   the
					#  <udig> of the rolled udig will be in
					#  the next wrap set as a "roll" blob
					#  request record.

	After the initial request by the client, all request replies are just
	'ok' or 'no', where 'no' is always the last reply. See below for the
	exact protocol.

	On the blob server, named "bio4d", for each correct client request a
	single traffic record describing the request is written to the file

		$BLOBIO_ROOT/spool/bio4d.brr

	Those traffic records are named "Blob Request Records" and abbreviated
	as BRR.  The BRR record format is modeled after the Call Detail Record
	from the telecommunications industry:

		https://en.wikipedia.org/wiki/Call_detail_record

	The format of a single BRR record is a line of seven tab separated
	fields: i.e, the typical unixy ascii record:

		start_time		#  RFC3339 nanosec:		\
					#      YYYY-MM-DDThh:mm:ss[.ns][+-]HH:MM
		transport		#  [a-z][a-z0-9]{0,7}~[[:graph:]]{1,128}
		verb			#  get|put|take|give|eat|wrap|roll[.u63]
		algorithm:digest	#  [a-z][a-z0-9]{0,7}:		\
					#	[[:graph:]]{32,192}
		chat_history		#  ok/no handshake between server&client
		blob_size		#  unsigned 63 bit long <= 2^63
		wall_duration		#  wall duration in 32 bit sec.ns>=0

	All characters are ascii.  Most important is that each client request
	is associated with a request for a single blob described by a udig.
	Also, BRR records are always syntactally correct as described above.
	In other words, a request for a syntactically incorrect udig or an
	unknown verb will not generate a BRR record.

	The minimum size of a brr record is XX ascii chars

		(19+1+5)	+ 1 +	=   start time<tab>
		(1+1+1)		+ 1 +	>=  transport<tab>
		(3)		+ 1 +	>=  verb<tab>
		(1+1+32)	+ 1 +	>=  udig<tab>
		(2)		+ 1 +	>=  chat history<tab> 
		(1)		+ 1 +	>=  blob size<tab>
		(1)		+ 1	>=  wall duration   #  newline required

	The maximum size of a blob request record is 456 ascii chars.

		(19+1+9+1+5)	+ 1 +	=   start time<tab>
		(8+1+196)	+ 1 +	<=  transport<tab>
		(4 + 1 + 19)	+ 1 +	<=  verb[.ui63]<tab>
		(8+1+128)	+ 1 +	<=  udig<tab>
		(8)		+ 1 +	<=  chat history<tab>
		(19)		+ 1 +	<=  blob size<tab>
		(10+1+10)	+ 1	<=  wall duration   #  newline required

		<= 456 bytes

	Since the length is <= 456 bytes, a BRR record can be broadcast in an
	UDP packet, with a "insured" atomic payload of 508 bytes.
	The BRR record format is cast in saphire.

Protocol Flow:
 	>get udig\n		# request for blob by udig
	  <ok\n[bytes][close]	#   server sends bytes of blob
	  <no\n[close]		#   server can not honor request
 	>get.5432 udig\n	# request for blob of size 5432 bytes by udig
	  <ok\n[5432 bytes][close]	#   server sends bytes of blob
	  <no\n[close]		#   server can not honor request

 	>put udig\n		# request to put blog matching a udig
	  <ok\n			#   server ready for blob bytes
	    >[bytes]		#     send bytes to server
	      <ok\n[close]	#       accepted bytes for blob
	      <no\n[close]	#       rejects bytes for blob
	  <no\n			#   server can not honor request
 	>put.5432 udig\n	# request to put blog of size 5432 and udig
	  <ok\n			#   server ready for blob bytes
	    >[5432 bytes]	#     send bytes to server
	      <ok\n[close]	#       accepted bytes for blob
	      <no\n[close]	#       rejects bytes for blob
	  <no\n			#   server can not honor request

  	>take udig\n		# request for blob by udig
  	  <ok\n[bytes]		#   server sends blob bytes
  	    >ok\n		#     client has the blob
	      <no[close]	#       server may not forget the blob
	      <ok[close]	#       server may forget the blob
	    >no\n[close		#     client rejects the taken blob
	  <no\n[close]		#   server can not forget blob
  	>take.5432 udig\n	# request for blob of 5432 bytes by udig
  	  <ok\n[bytes]		#   server sends 5432 blob bytes
  	    >ok\n		#     client has the blob
	      <no[close]	#       server may not forget the blob
	      <ok[close]	#       server may forget the blob
	    >no\n[close		#     client rejects the taken blob
	  <no\n[close]		#   server can not forget blob

 	>give udig\n		# request to put blob matching a udig
	  <ok\n			#   server ready for blob bytes
	    >[bytes]		#     send digested bytes to server
  	      <ok\n		#   server accepts the bytes
  	        >ok[close]	#     client probably forgets blob
  		>no[close]	#     client might remember the blob
	      <no\n[close]	#   server rejects the blob
  	  <no\n[close]		#   server rejects blob give request
 	>give.5432 udig\n	# request to put blob of 5432 bytes by udig
	  <ok\n			#   server ready for blob bytes
	    >[bytes]		#     send digested bytes to server
  	      <ok\n		#   server accepts the bytes
  	        >ok[close]	#     client probably forgets blob
  		>no[close]	#     client might remember the blob
	      <no\n[close]	#   server rejects the blob
  	  <no\n[close]		#   server rejects blob give request

	>eat udig\n		# client requests server to verify blob
	    <ok\n[close]	#   blob exists and has been verified
	    <no\n[close]	#   server was unable to digest the blob

	>wrap			# request udig set of traffic logs
	    <ok\nudig\n[close]	#   udig of set of traffic logs
	    <no\n[close]	#   no logs available

	>roll udig		# udig set of traffic logs to forget
	    <ok\n[close]	# logs forgotten by server.  typically
	    			# a wrap set.
	    <no\n[close]	# not all logs in set forgotten, or other error

	Problematically, the <verb>.ui63 request does leak info about the blob.
	also, probing for support of the <byte-count> extension could be
	"solved" by requiring all bio4 servers to contain the "empty" blob
	("epsilon knowledge").

	to illustrate probing for byte count support, consider the empty
	blob for sha digest:

		sha:da39a3ee5e6b4b0d3255bfef95601890afd80709

	the chat probe looks like

		> get sha:da39a3ee5e6b4b0d3255bfef95601890afd80709
		< ok

		#  can never remove the empty blob
		> take sha:da39a3ee5e6b4b0d3255bfef95601890afd80709
		< no

		> get.0 sha:da39a3ee5e6b4b0d3255bfef95601890afd80709
		< no

	proves that the server does not support <byte-count> for the get
	request.  and

		> get.0 sha:da39a3ee5e6b4b0d3255bfef95601890afd80709
		< ok

	proves <byte-count> is supported for get request.

	As blobio evolves, forward facing, untrusted applications will rarely
	speak the bio4 protocol.  instead, a security oriented gateway protocol
	will sit in front of the bio4 services, such as http or a REST api.
	in other words, all blob gateways are caches and a bio4 service is
	service of last resort.

Protocol Questions:
	- clarify the meaning of "transport" field in blob request record.
	   for networks, transport are roughly the protocol, request endpoint
	   and the negotiated endpoint.

	- what about forbidding ':' as a digest character?

        - think about a "sum" command that allows the server to formally prove
	   the existence of a set of blobs, thus verifying the server is not
	   lying about storage.  the obvious technique would be for the client
	   to send the udig of a set of blobs unknown to the server, for which
	   the server is expected to digest the cancatenation of the blobs.

	- reimplement wrap as

		wrap <udig>

	   the idea is that the final record of the wrap set will always contain
	   <udig>, creating a kind of audit trail.  also insures no wrap set is
	   empty and sequential wraps could always return different udigs if the
	   given udig was unique. in shell parlance

	  	test $(blobio wrap <udigA>) = $(blobio wrap <udigB>)

	   is always false, even when udigA = udigB.

Billing Model Questions:
	- what to measure per billing cycle:
		total count of blobs stored
		total bytes stored
		total bytes transfered on public interface
		avg transfer bytes/duration > X
			for blob size > Y
		total number of eat requests/proof of retrievability

		should a small numbers of plans exist or should a formula
		determine cycle charges?

		what about overages? a mulligan is nice for customer
		satisfaction but rolling to next higher plan is draconian.

	- duration of storage: per blob or per plan?

	- should the blob request record include a "duration to first byte",
	  which measures the difference between start_time and time after the
	  first read or write to the client.  the time to first byte can be
	  observed on the network, sort of.  the sender will see a slightly
	  different time to first byte than the receiver, as with the
	  start_time.

Note:
	- should "get" verb include a reply?

		>get
		 <ok ... bytes
		 >ok

	- what would eat.ui63 mean?  seems like would be useful with trusted'
	  network.

	- investigate fsverity in linux kernel.
		https://www.kernel.org/doc/html/latest/filesystems/fsverity.html
	- also, what about leap-seconds (:23:59:60) in starttime.

	- requiring nanosecs to always be 9digits does not agree with
	  the RFC.

	- a "wrap" -> "no" has no <udig> in brr, implying the minimum is too
	  big.

	- where is syntax for BLOBIO_SERVICE defined?

	- should the term "fault" be replaced with "panic", in the code?

	- on a brr record, should transport+start_time be required to be unique?

	- move pgsql data type public.udig to schema blobio.udig?!

	- rename fs_sha and fs_btc20 to posix_sha and posix_btc20?

	- follow project for atomic write in linux
		https://lwn.net/Articles/789600/

	- casting text to udig_sha fails when prefixed by sha:

		--  OK
		select '44e946a2889b295eeb0b9d95efe34625cc830dde'::udig_sha;

		-- FAILS
		select 'sha:44e946a2889b295eeb0b9d95efe34625cc830dde'::udig_sha;
	
	  why not cast both versions?

	- move log/*.[fx]dr files to spool and, after wrapping, to data/

	- Should the variable BLOBIO_ROOT be renamed BLOBIO_INSTALL or
	  BLOBIO_DIST?  Currently BLOBIO_ROOT means the actual install 
	  directory and not the parent, which is the correct root.

	- add option to bio4d to limit size of accepted blob

	- investigate linux splice() system call.

	- think about separate bio4d processes sharing the same data/
	  file system but otherwise separate.  or, explore union file systems.

	- make the postgresql udig data type be an extension

	- prove that every brr log contains a successful "wrap" other than the
	  the first brr log.  critical to prove this.

	- two quick wraps in a row could generate the same wrap set?

	- should BRR log files always have unique digests, modulo other servers.
	  perhaps the first put request in the wrapped brr is a blob uniquely
	  summarizing the state of the server and the current wrap,request.

	- contemplate running the server such that the empty blob always exists

	- think about adding fadvise to file system blob reads.
          seems natural since every open of blob file will read the entire blob.
	- create an is_empty() function in pgsql udig datatype.

	- add support for unix domain socket, std{in,out}, inetd, and libevent

	- upon server start up on mac os x test for case sensitive file system

	     see http://www.mail-archive.com/wine-devel@winehq.org/msg66830.html

	  another idea under macosx would be to convert blob file path to lower
	  case.

	- rewrite makefile for client only install, without golang requirement

	- should the postgres datatype include a function is_zero_size(udig)?

	- the postgres udig data type ought to be in the blobio schema.

	- merge fdr2sql & cron-fdr2pg into flowd

	- enable/disable specific commands get/put/give/take per server/per
	  subnet

	- named pipes to flowd as another tail source

	- signal handling needs to be pushed to main listen loop or cleaned
 	  up with sigaction().
